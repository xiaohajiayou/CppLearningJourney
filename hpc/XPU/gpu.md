
## wrap调度器
wrap调度器负责寄存器资源的分发。DSA由于任务特化，按照具体任务定死资源分配，灵活性较差。
## spmd编程模式
NPU难以实现CUDA的SIMT范式，主要是受硬件架构基础、编程生态发展、应用场景定位和技术演进路径的影响。这使得NPU在通用计算的灵活性和编程易用性上与CUDA的SIMT范式存在差距。
1. **硬件架构差异**：传统NPU多基于SIMD架构，数据处理以向量指令为主，缺乏SIMT中线程独立执行和灵活控制流的硬件支持。像华为昇腾NPU微架构中，控制单元占比极少，大部分面积给了计算和存储单元，导致其软件栈复杂，难以实现SIMT范式下高效的通用调度和编程易用性。而CUDA的SIMT架构允许线程独立执行，每个线程能访问自己的寄存器、从不同地址读写数据并遵循不同控制流路径，硬件动态组织线程成warp来加速计算并行度，在硬件设计上解决了大部分SIMD数据路径的流水编排问题 。
2. **编程生态与易用性**：CUDA拥有成熟丰富的编程生态，围绕GPU加速的后端库（如cublas、cudnn等）众多，配合灵活的编程接口，开发者能轻松实现和优化新AI模型。NPU的编程模型和工具链相对不成熟，很多NPU为保持硬件设计的独立性和封闭性，对公众开放的编程接口有限，即便有类似CUDA的编程语言（如英特尔Gaudi NPU的tpc-c ），但在整个软件生态的完整性和易用性上仍与CUDA有较大差距。
3. **应用场景与设计初衷**：NPU专为加速神经网络计算设计，聚焦于深度学习中的张量计算和矩阵运算，如卷积、矩阵乘法等特定操作。其设计主要针对特定的AI任务优化，在功能实现上较为固定，不像CUDA的SIMT范式追求通用计算场景下的灵活性。例如，部分NPU硬件设计可能仅支持特定的数据格式（如FP8 ），若未来应用需求变化，很难对其进行更改，限制了NPU在更广泛场景下实现类似SIMT范式的灵活性 。
4. **技术演进与历史积累**：NVIDIA在GPU领域发展多年，CUDA的SIMT范式是其长期技术演进和优化的成果。从早期显卡发展到如今的GPU计算，NVIDIA逐步构建起完善的控制单元和编程模型。NPU作为新兴的AI加速硬件，发展时间较短，在技术积累和演进上还未达到CUDA的成熟度，难以在短时间内实现类似SIMT范式这样复杂且成熟的编程模型 。 
## ai芯片思考
- **模型快速增长，芯片发展滞后**：AI网络模型越来越大，所需内存空间和算力平均每年增长50%，而芯片设计、部署及优化需约5年，发展速度远跟不上模型增长，如GPT3模型参数较GPT2最小模型提高近百倍，而AI芯片性能增长乏力。
- **芯片需支持通用新模型**：深度学习领域发展快，网络模型架构不断变化，从分类到图像分割再到视频生成，从以分类为主到CNN、BT网络百花齐放，再到transformer架构独占鳌头。芯片设计周期长，需支持通用新模型以适应网络模型快速演进。
- **生产部署需多租户技术**：实际生产部署中，需要多租户技术让多个模型或同一模型的多个实验等在单张卡牌、单个节点上运行，这需要算力切分、显存虚拟化等技术，而很多MPU、TPU缺乏这些功能，GPU在这方面有多年积累。
- **AI芯片中缓存和显存的需求**：希望AI芯片有较大的缓存（SM）和较快速度的显存（如HBM）。不少模型需要的缓存远大于单个芯片最大SM，一些芯片希望利用SVM解决大部分任务，减少数据搬运时间。
- **内存比计算量更重要**：微处理器最大瓶颈是能耗，访问内存的能耗很高。AI芯片通过浮点运算单元分摊内存访问开销，开发者也常通过减少内存访问优化模型，如重计算。英伟达和谷歌TPU在相关架构设计上有相应措施。
- **DSA需专业且灵活**：针对AI的TPU、MPU等DSA既要专业优化神经网络和大模型，又要保持灵活度。训练场景比推理更复杂，计算量大、运算多、需更大存储空间，对数据格式、指令流水和可编程性要求高，需要配套的编译器和软件，英伟达在这方面表现出色，国内华为在训练芯片方面有一定能力。
- **半导体技术发展需做好选型**：半导体技术发展速度不同，计算逻辑进步快，芯片布局发展较慢。显存中SMHBN比dd4和GDDR6速度更快、能效更高，需根据具体情况做好选型，如数据操作格式，选不好会影响速度。
- **编译器优化与AI应用需兼容**：编译器需对AI模型分析优化，使其在硬件上充分发挥性能，处理并行技术和内存传输管理等问题。但目前DSA软件栈不成熟，编译器优化带来的性能提升可能不如人为调优算子，且像flash attention通过减少内存访问提升速度的方式，编译器难以承载，需深度编译优化。